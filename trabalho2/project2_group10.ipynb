{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IART - 2ND GROUP PROJECT (CAR INSURANCE CLAIM PREDICTION)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- We were proposed to create a supervised machine learning model in order to predict whether the policyholder files a claim in the next 6 months or not.\n",
    "\n",
    "- To do that, we had a database which contains various informations such as policy tenure, age of the car, age of the car owner, the population density of the city, make and model of the car, power, engine type, etc, and some of them might be relevant to our model use to make the prediction.\n",
    "\n",
    "- Firstly, we had to sanitize our database so that it could be free from outliers and irrelevant attributes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "- At first we made a describe of the database, so that we could know what attributes and its values it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "car_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Learn more about the training data attributes\n",
    "print(car_data.head())\n",
    "print(car_data.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we have also removed the attributes that were not relvant for the prediction, such as ID's, cars' torques, etc, and checked if the database was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unuseful columns\n",
    "car_data = car_data.drop(columns=['policy_id', 'is_adjustable_steering', 'is_tpms',\n",
    "                                  'is_parking_camera', 'rear_brakes_type', 'cylinder',\n",
    "                                  'transmission_type', 'turning_radius', 'is_rear_window_washer',\n",
    "                                  'is_power_door_locks', 'is_central_locking',\n",
    "                                  'is_day_night_rear_view_mirror'])\n",
    "\n",
    "# Check transformed data\n",
    "car_data.to_csv('./data/cleansed_car_data.csv', index=False)\n",
    "print(car_data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After that, we filtered for null values. In our case, fortunately, we dind't had in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_null_values = car_data.isnull().sum()\n",
    "print(n_of_null_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Besides that, we removed the duplicated rows, so that we prevented from using repeated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries\n",
    "car_data = car_data.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, we target encoded the training data, so that it would be better suited for the training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Target encode the training data, to better suit the training models\n",
    "for column in car_data.columns:\n",
    "    if dict(car_data.dtypes)[column] == 'object':  # If it is a value of type string or categorical variable     \n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        car_data[column] = label_encoder.fit_transform(car_data[column])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afterwards, we took all age related columns and verified if the values were between 0 and 1, since the dataset description indicated that all age columns were normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all age values not between 0 and 1 \n",
    "subset = car_data[~car_data['age_of_policyholder'].between(0, 1)]\n",
    "print(\"\\nNumber of entries where policy holder age not between 0 and 1: \"+str(len(subset)))\n",
    "# car_data = car_data[car_data['age_of_policyholder'].between(0, 1)]\n",
    "\n",
    "subset = car_data[~car_data['age_of_car'].between(0, 1)]\n",
    "print(\"\\nNumber of entries where age of car not between 0 and 1: \"+str(len(subset)))\n",
    "# car_data = car_data[car_data['age_of_car'].between(0, 1)]\n",
    "\n",
    "subset = car_data[~car_data['policy_tenure'].between(0, 1)]\n",
    "print(\"\\nNumber of entries where policy tenure not between 0 and 1: \"+str(len(subset)))\n",
    "car_data = car_data[car_data['policy_tenure'].between(0, 1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we removed outliers using the Z-Score and Inter Quartile Range methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_outliers_zscore(data):\n",
    "    outliers = []\n",
    "    threshold = 3\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    for idx, value in enumerate(data):\n",
    "        z_score = (value - mean) / std\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(idx)\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(data):\n",
    "    outliers = []\n",
    "    data = sorted(data)\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    IQR = q3 - q1\n",
    "    lower_bound = q1 - (1.5 * IQR)\n",
    "    upper_bound = q3 + (1.5 * IQR)\n",
    "    for idx, value in enumerate(data):\n",
    "        if value < lower_bound or value > upper_bound:\n",
    "            outliers.append(idx)\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using Z-Score and Inter Quartile Range\n",
    "car_data = car_data.reset_index(drop=True)\n",
    "\n",
    "for column in ['population_density', 'fuel_type', 'displacement', 'length','width','height','gross_weight']:\n",
    "    outliers = detect_outliers_zscore(car_data[column])\n",
    "    print(\"\\nNumber of Z-score outliers in \"+column+\" column: \"+str(len(outliers)))\n",
    "    car_data.drop(outliers)\n",
    "\n",
    "    outliers = detect_outliers_iqr(car_data[column])\n",
    "    print(\"Number of IQR outliers in \"+column+\" column: \"+str(len(outliers)))\n",
    "    car_data.drop(outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To finish up, we scaled the data so that all columns would be between values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(car_data)\n",
    "car_data = pd.DataFrame(scaled_data, columns=car_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After it all, we have cleansed the database. Here is its plot (only some columns included):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sb.pairplot(car_data[['policy_tenure','age_of_car','age_of_policyholder','area_cluster', 'is_claim']], hue='is_claim')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "- We realised that some classes were imbalanced and some features should be separated from the target and feature variables. We did that in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into target and feature variables\n",
    "car_data_target = car_data['is_claim']\n",
    "car_data_features = car_data.drop('is_claim', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to check the attibutes' imbalance, we count the number of 1's and compared with the number's of 0's. \n",
    "- With this, we concluded that there was an imbalance in the target variable, due to the overwhelming amount of 0's compared to 1's. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance - the results indicate that there is a clear imbalance in the target variable, an overwhelming amount of 0's compared to 1's\n",
    "print(car_data_target.value_counts(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "- We also separated data from training to testing, in favor of seed's consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Separate data into training and testing, provide seed for consistency\n",
    "car_data_features_train, car_data_features_test, car_data_target_train, car_data_target_test = model_selection.train_test_split(car_data_features, car_data_target, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to attenuate attributes' imbalance, we decided to use weights. In this case, 'balanced' == n_samples / (n_classes * np.bincount(is_claim))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Correct class imbalance using class weights. 'balanced' == n_samples / (n_classes * np.bincount(is_claim))\n",
    "weights = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(car_data_target_train), y = car_data_target_train)\n",
    "print(weights[0], weights[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for the model\n",
    "\n",
    "- After the database clean-up, we had to decide which algorithm our model should use. For that, we trained 3 separate algorithms: Decision Trees, K-NN and Logistic Regression, and chose which had the better results. We chose these algorithms, since they are used in classification problems.\n",
    "\n",
    "- The choice of the algorithm was based on the results for different performance measures, such as accuracy, recall and F1-measure. We also applied a confusion matrix and ROC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "- For this part we used a Decision Tree classifier to predict the target attribute in the training data. After that, we calculated the performance measures for different characteristics of the classifier and chose the best ones. In the snippet code below is shown what was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we created a Classifier with all possible attributes combinations.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [1, 2, 3, 4, 5],\n",
    "    'max_features': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "# The Grid Search, with the help of the stratikied K-fold, will show the best parameters \n",
    "# to use for the Decision Tree\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10)\n",
    "\n",
    "gs_dt_cv = GridSearchCV(dtc, param_grid=grid, cv=cross_validation)\n",
    "gs_dt_cv.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "print('For Decision Tree:')\n",
    "print('Best score: {}'.format(gs_dt_cv.best_score_))\n",
    "print('Best parameters: {}'.format(gs_dt_cv.best_params_))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After that, we used the classifier with the best characteristics and meausred its performance. Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This part will show all the performance measures (either graphics, or analytics) of the Decision\n",
    "# Tree Classifier with the best parameters.\n",
    "\n",
    "print('For decision tree:')\n",
    "\n",
    "dtc = DecisionTreeClassifier(\n",
    "    criterion='gini', max_depth=5, max_features=3, splitter='best', class_weight='balanced', \n",
    "    random_state=1)\n",
    "\n",
    "dtc.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "dtc.score(car_data_features_train, car_data_target_train)\n",
    "\n",
    "# Graphical performance measures for Decision Tree (ROC and Confusion Matrix)\n",
    "\n",
    "dtc_pred = dtc.predict(car_data_features_train)\n",
    "\n",
    "cmat = confusion_matrix(car_data_target_train, dtc_pred, labels=dtc.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmat, display_labels=dtc.classes_)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_pred_proba = dtc.predict_proba(car_data_features_train)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(car_data_target_train ,target_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Analytical performance measures for Decision Tree\n",
    "\n",
    "accuracy = accuracy_score(car_data_target_train,dtc_pred)\n",
    "\n",
    "precision = precision_score(car_data_target_train,dtc_pred, average=\"macro\")\n",
    "\n",
    "recall = recall_score(car_data_target_train,dtc_pred, average=\"macro\")\n",
    "\n",
    "f1 = f1_score(car_data_target_train,dtc_pred, average=\"macro\")\n",
    "\n",
    "dt_performance = [f1, fpr, tpr, precision, recall, accuracy]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "\n",
    "print(f\"F1 Accuracy: {f1:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In fact, the precision measures are low, specially the F1-accuracy and it can be confirmed due to the incorrect predictions that the policyholdes will not claim the insurance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN\n",
    "\n",
    "- We did the similar for the K-NN algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Similarly to what was done in the Decision Tree, we generate all the possible \n",
    "# KNeighbors Classifiers, varying their parameters.\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "knn_grid = {\n",
    "'weights': ['uniform', 'distance'],\n",
    "'n_neighbors': [10,25,50],\n",
    "'n_jobs': [10,25,50],\n",
    "}\n",
    "\n",
    "# After that, we the GridSearch will select the K-NN classifier with the best parameter,\n",
    "# with the help of the cross validation created before.\n",
    "\n",
    "gs_kn_cv = GridSearchCV(knc, param_grid=knn_grid, cv=cross_validation)\n",
    "gs_kn_cv.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "print('For K-NN')\n",
    "print('Best score: {}'.format(gs_kn_cv.best_score_))\n",
    "print('Best parameters: {}'.format(gs_kn_cv.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part will show all the performance measures (either graphics, or analytics) of the\n",
    "# K-NN Classifier with the best parameters.\n",
    "\n",
    "print('For K-NN:')\n",
    "\n",
    "knc = KNeighborsClassifier(n_jobs=10, n_neighbors=25, weights='uniform')\n",
    "\n",
    "knc.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "knc.score(car_data_features_train, car_data_target_train)\n",
    "\n",
    "knn_pred = knc.predict(car_data_features_train)\n",
    "\n",
    "# Graphical performance measures for Decision Tree (ROC and Confusion Matrix)\n",
    "\n",
    "cmat = confusion_matrix(car_data_target_train, knn_pred, labels=knc.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmat, display_labels=knc.classes_)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_pred_proba = knc.predict_proba(car_data_features_train)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(car_data_target_train ,target_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Analytical performance measures for Decision Tree\n",
    "\n",
    "accuracy = accuracy_score(car_data_target_train,knn_pred)\n",
    "\n",
    "precision = precision_score(car_data_target_train,knn_pred, average=\"macro\")\n",
    "\n",
    "recall = recall_score(car_data_target_train, knn_pred, average=\"macro\")\n",
    "\n",
    "f1 = f1_score(car_data_target_train,knn_pred, average=\"macro\")\n",
    "\n",
    "knn_performance = [f1, fpr, tpr, precision, recall, accuracy]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "\n",
    "print(f\"F1 Accuracy: {f1:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we looked the precisions measures are slighly better in the K-NN, it could give us a (wrong) idea that the K-NN model had a better performance. However, and thanks to the confusion matrix plots, we concluded that the model was always predicting that a policeholder will not claim an insurance. This will lead to the besta values for true and false positives value (maximum and 0, respectively), but the false negatives had the maximum value possible and, as result, it does not complete the objective, which is to predict when they will claim it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "- And the same for Logistic Regression. In this part, it appeared some errors due to the combination of values of penalty that were not accepted using l2 and elasticnet. Here are the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Similarly to what was done in the Decision Tree and in the K-NN, we generate all the possible \n",
    "# Logistic Regression Classifiers, varying their parameters of the grid.\n",
    "\n",
    "lrc = LogisticRegression()\n",
    "\n",
    "lrc_grid = {\n",
    "'penalty': ['l1','l2','elasticnet'],\n",
    "'l1_ratio': [1, 0, 0.2, 0.5],\n",
    "'tol': [0.2, 0.5]\n",
    "}\n",
    "\n",
    "#The Grid Search with cross-validation will select the Logistic Regression Classifiers with \n",
    "# the best parameters to predict.\n",
    "\n",
    "gs_lrc_cv = GridSearchCV(lrc, param_grid=lrc_grid,cv=cross_validation)\n",
    "gs_lrc_cv.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "print('For Logistic Regression:')\n",
    "print('Best score: {}'.format(gs_lrc_cv.best_score_))\n",
    "print('Best parameters: {}'.format(gs_lrc_cv.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Here the Logistic Regression's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For Logistic Regression:')\n",
    "\n",
    "# This part will show all the performance measures (either graphics, or analytics) of the\n",
    "# Logistic Regression Classifier with the best parameters.\n",
    "\n",
    "lrc = LogisticRegression(penalty='l2', tol=0.2, class_weight='balanced', random_state=1)\n",
    "\n",
    "lrc.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "lrc.score(car_data_features_train, car_data_target_train)\n",
    "\n",
    "log_pred = lrc.predict(car_data_features_train)\n",
    "\n",
    "# Graphical performance measures for Decision Tree (ROC and Confusion Matrix)\n",
    "\n",
    "cmat = confusion_matrix(car_data_target_train, log_pred, labels=lrc.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmat, display_labels=lrc.classes_)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_pred_proba = lrc.predict_proba(car_data_features_train)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(car_data_target_train ,target_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Analytical performance measures for Decision Tree\n",
    "\n",
    "accuracy = accuracy_score(car_data_target_train,log_pred)\n",
    "\n",
    "precision = precision_score(car_data_target_train,log_pred, average=\"macro\")\n",
    "\n",
    "recall = recall_score(car_data_target_train, log_pred, average=\"macro\")\n",
    "\n",
    "f1 = f1_score(car_data_target_train, log_pred, average=\"macro\")\n",
    "\n",
    "lr_performance = [f1, fpr, tpr, precision, recall, accuracy]\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "\n",
    "print(f\"F1 Accuracy: {f1:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the ratings are low, and is predicting a lot that a policyholder will not claim the insurance when they will (false-negatives). However, this model had the best performance of the 3 used, since it had the best perfomance measures than the Decision Tree (DT) and K-NN, and a similar confusion matrix compared to the DT one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After that, we used the weights in order to compare the various algorithms and choose one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The voting classifier will ensure the use of the weights created in the classifiers \n",
    "# and choose the one which has the best accuracy. In this case, all of them have the same\n",
    "# average accuracy.\n",
    "\n",
    "ensemble = VotingClassifier([('dt', dtc), ('knn',knc)], voting='soft', weights=[weights[0], weights[1]])\n",
    "\n",
    "ensemble.fit(car_data_features_train, car_data_target_train)\n",
    "\n",
    "pred = ensemble.predict(car_data_features_train)\n",
    "\n",
    "score = accuracy_score(car_data_target_train, pred)\n",
    "\n",
    "print('Weighted Avg Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we could see and analysing the results of the measures and plots, we decided, to use the Logistic Regression Model for our prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "\n",
    "- After that, we made the learning curve algorithm for the Logistice Regression, in order to know in which point it has over-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Curve\n",
    "car_train_sizes, car_train_scores, car_test_scores = learning_curve(lrc,\n",
    "                car_data_features_train, car_data_target_train, scoring = 'accuracy', n_jobs = 1, random_state=1)\n",
    "\n",
    "train_mean = np.mean(car_train_scores , axis = 1)\n",
    "train_std = np.std(car_train_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(car_test_scores, axis=1)\n",
    "test_std = np.std(car_test_scores, axis=1)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.plot(car_train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(car_train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(car_train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(car_train_sizes , test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final analysis and conclusions\n",
    "\n",
    "- With the results of the previous analysis, we diecided to use the Logistic Regression, since it has the best performance (it has a better accurancy than the Decision Tree's and also have a constant learning curve), even though K-NN has a better accuracy (K-NN always predicting 0, so it has no FP, but we know it is not a good prediction).\n",
    "\n",
    "- So let's make the similar, but using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did a similar Logistic Regression Classifier for the test data, with the same parameters\n",
    "# as the one used in the training data. We also printed the performance measures.\n",
    "\n",
    "lrc = LogisticRegression(l1_ratio=1, penalty='l2', tol=0.2, class_weight='balanced', random_state=1)\n",
    "\n",
    "lrc.fit(car_data_features_test, car_data_target_test)\n",
    "\n",
    "lrc.score(car_data_features_test, car_data_target_test)\n",
    "\n",
    "log_pred = lrc.predict(car_data_features_test)\n",
    "\n",
    "# Created the Confusion Matrix and the ROC.\n",
    "\n",
    "cmat = confusion_matrix(car_data_target_test, log_pred, labels=lrc.classes_)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmat, display_labels=lrc.classes_)\n",
    "\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "target_pred_proba = lrc.predict_proba(car_data_features_test)[::,1]\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(car_data_target_test ,target_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Created the analytical performance measures.\n",
    "\n",
    "accuracy = accuracy_score(car_data_target_test,log_pred)\n",
    "\n",
    "precision = precision_score(car_data_target_test,log_pred, average=\"macro\")\n",
    "\n",
    "recall = recall_score(car_data_target_test, log_pred, average=\"macro\")\n",
    "\n",
    "f1 = f1_score(car_data_target_test, log_pred, average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.5f}\")\n",
    "\n",
    "print(f\"F1 Accuracy: {f1:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected, we concluded that the model has a similar behaviour as the training. However, having a reasonable accuracy does not mean having a good prediction model (as we can see for the value of F1-accuracy).\n",
    "\n",
    "- Due to the huge number of false-negatives, we can say that our model is good to predict when a policeholder will not claim the prediction, but we can't say if is going to predict when he/she will."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
